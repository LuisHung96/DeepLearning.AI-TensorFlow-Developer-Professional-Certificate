{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP+a3FeucxtjphalLKpLnl0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Ungraded Lab: Generating Sequences and Padding\n","\n","In this lab, you will look at converting input sentences into numeric sequences. Similar to images in the previous course, you need to prepare text data with uniform size before feeding it to your model. You will see how to do these in the next sections.\n","\n","**IMPORTANT NOTE:** This notebook is designed to run as a Colab. Running it on your local machine might result in some of the code blocks throwing errors."],"metadata":{"id":"5HrICLKz1zIe"}},{"cell_type":"markdown","source":["## Text to Sequences\n","\n","In the previous lab, you saw how to use the `TextVectorization` layer to build a vocabulary from your corpus. It generates a list where more frequent words have lower indices."],"metadata":{"id":"68qMlugH2dOj"}},{"cell_type":"code","source":["import tensorflow as tf\n","\n","# Sample inputs\n","sentences = [\n","    'I love my dog',\n","    'I love my cat',\n","    'You love my dog!',\n","    'Do you think my dog is amazing?'\n","    ]\n","\n","# Initialize the layer\n","vectorize_layer = tf.keras.layers.TextVectorization()\n","\n","# Compute the vocabulary\n","vectorize_layer.adapt(sentences)\n","\n","# Get the vocabulary\n","vocabulary = vectorize_layer.get_vocabulary()\n","\n","# Print the token index\n","for index, word in enumerate(vocabulary):\n","  print(index, word)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tyTtv2I_2elK","executionInfo":{"status":"ok","timestamp":1739965615925,"user_tz":240,"elapsed":6994,"user":{"displayName":"Luis Alfredo Hung Araque","userId":"00964424177241549147"}},"outputId":"cc308080-fc56-4cb3-ef5b-8abe2f50d35a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0 \n","1 [UNK]\n","2 my\n","3 love\n","4 dog\n","5 you\n","6 i\n","7 think\n","8 is\n","9 do\n","10 cat\n","11 amazing\n"]}]},{"cell_type":"markdown","source":["You can then use the result to convert each of the input sentences into integer sequences. See how that's done below given a single input string."],"metadata":{"id":"u6cVIal5filf"}},{"cell_type":"code","source":["# String input\n","sample_input = 'I love my dog'\n","\n","# Convert the string input to an integer sequence\n","sequence = vectorize_layer(sample_input)\n","\n","# Print the result\n","print(sequence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FDxKXKFw2uy6","executionInfo":{"status":"ok","timestamp":1739965615926,"user_tz":240,"elapsed":6,"user":{"displayName":"Luis Alfredo Hung Araque","userId":"00964424177241549147"}},"outputId":"6f0a4681-c19e-47fc-8075-6c59cc6f7bd7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tf.Tensor([6 3 2 4], shape=(4,), dtype=int64)\n"]}]},{"cell_type":"markdown","source":["As shown, you simply pass in the string to the layer which already learned the vocabulary, and it will output the integer sequence as a `tf.Tensor`. In this case, the result is `[6 3 2 4]`. You can look at the token index printed above to verify that it matches the indices for each word in the input string.\n","\n","For a given list of string inputs (such as the 4-item `sentences` list above), you will need to apply the layer to each input. There's more than one way to do this. Let's first use the `map()` method and see the results."],"metadata":{"id":"vRtPSxrE41Ty"}},{"cell_type":"markdown","source":["### First form"],"metadata":{"id":"P9iRUlXE5GE5"}},{"cell_type":"code","source":["# Convert the list to tf.data.Dataset\n","sentences_dataset = tf.data.Dataset.from_tensor_slices(sentences)\n","\n","# Define a mapping function to convert each sample input\n","sequences = sentences_dataset.map(vectorize_layer)\n","\n","# Print the integer sequences\n","for sentence, sequence in zip(sentences, sequences):\n","  print(f'{sentence} ---> {sequence}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j5A5VvGr5jmi","executionInfo":{"status":"ok","timestamp":1739965615926,"user_tz":240,"elapsed":5,"user":{"displayName":"Luis Alfredo Hung Araque","userId":"00964424177241549147"}},"outputId":"8a134986-40fc-43c6-8b7a-00d40b2912c1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["I love my dog ---> [6 3 2 4]\n","I love my cat ---> [ 6  3  2 10]\n","You love my dog! ---> [5 3 2 4]\n","Do you think my dog is amazing? ---> [ 9  5  7  2  4  8 11]\n"]}]},{"cell_type":"markdown","source":["As you can see, each sentence is successfully transformed into an integer sequence. The problem with this is they have varying lengths so it cannot be consumed by the model right away."],"metadata":{"id":"EAztOW0y6e0T"}},{"cell_type":"markdown","source":["## Padding\n","\n","You can get a list of varying lengths to have a uniform size by padding or truncating tokens from the sequences. Padding is more common to preserve information.\n","\n","Recall that your vocabulary reserves a special token index `0` for padding. It will add that token (called post padding) if you pass in a list of string inputs to the layer. See an example below. Notice that you have the same output as above but the integer sequences are already post-padded with `0` up to the length of the longest sequence."],"metadata":{"id":"DRAwW1qQ6gHR"}},{"cell_type":"code","source":["# Apply the layer to the string input list\n","sequences_post = vectorize_layer(sentences)\n","\n","# Print the results\n","print('INPUT:')\n","print(sentences)\n","print()\n","\n","print('OUTPUT:')\n","print(sequences_post)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uCk1zEhi7GTq","executionInfo":{"status":"ok","timestamp":1739965619483,"user_tz":240,"elapsed":379,"user":{"displayName":"Luis Alfredo Hung Araque","userId":"00964424177241549147"}},"outputId":"246c43c8-26b3-4a1b-b84f-ffdfd53343f2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["INPUT:\n","['I love my dog', 'I love my cat', 'You love my dog!', 'Do you think my dog is amazing?']\n","\n","OUTPUT:\n","tf.Tensor(\n","[[ 6  3  2  4  0  0  0]\n"," [ 6  3  2 10  0  0  0]\n"," [ 5  3  2  4  0  0  0]\n"," [ 9  5  7  2  4  8 11]], shape=(4, 7), dtype=int64)\n"]}]},{"cell_type":"markdown","source":["If you want pre-padding, you can use the [pad_sequences()](https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences) utility to prepend a padding token to the sequences. Notice that the `padding` argument is set to `pre`. This is just for clarity. The function already has this set as the default so you can opt to drop it."],"metadata":{"id":"Sg1PylCJ84gx"}},{"cell_type":"code","source":["# Pre-pad the sequences to a uniform length.\n","# You can remove the `padding` argument and get the same result.\n","sequences_pre = tf.keras.utils.pad_sequences(sequences, padding='pre')\n","\n","# Print the results\n","print('INPUT:')\n","[print(sequence.numpy()) for sequence in sequences]\n","print()\n","\n","print('OUTPUT:')\n","print(sequences_pre)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dcGd7D8xCRg0","executionInfo":{"status":"ok","timestamp":1739965849750,"user_tz":240,"elapsed":384,"user":{"displayName":"Luis Alfredo Hung Araque","userId":"00964424177241549147"}},"outputId":"0ad84476-4250-46e0-9057-07d8a5a6a3d1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["INPUT:\n","[6 3 2 4]\n","[ 6  3  2 10]\n","[5 3 2 4]\n","[ 9  5  7  2  4  8 11]\n","\n","OUTPUT:\n","[[ 0  0  0  6  3  2  4]\n"," [ 0  0  0  6  3  2 10]\n"," [ 0  0  0  5  3  2  4]\n"," [ 9  5  7  2  4  8 11]]\n"]}]},{"cell_type":"markdown","source":["If you switch the `padding` argument to `post`, you will arrive at the same result as applying the layer directly.\n","\n","The function also has a `maxlen` argument that you can use to truncate tokens from the sequences. By default, it will drop tokens in front. If you want to drop tokens at the other end, you will have to set the `truncating` argument to `post`."],"metadata":{"id":"wPakZEquDSRq"}},{"cell_type":"code","source":["# Post-pad the sequences and limit the size to 5.\n","sequences_post_trunc = tf.keras.utils.pad_sequences(sequences, maxlen=5, padding='post')\n","\n","# Print the results\n","print('INPUT:')\n","[print(sequence.numpy()) for sequence in sequences]\n","print()\n","\n","print('OUTPUT:')\n","print(sequences_post_trunc)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tNdLRshmD0_K","executionInfo":{"status":"ok","timestamp":1739969620429,"user_tz":300,"elapsed":235,"user":{"displayName":"Luis Alfredo Hung Araque","userId":"00964424177241549147"}},"outputId":"82a736cb-2419-4154-dd3b-0e97410792cc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["INPUT:\n","[6 3 2 4]\n","[ 6  3  2 10]\n","[5 3 2 4]\n","[ 9  5  7  2  4  8 11]\n","\n","OUTPUT:\n","[[ 6  3  2  4  0]\n"," [ 6  3  2 10  0]\n"," [ 5  3  2  4  0]\n"," [ 7  2  4  8 11]]\n"]}]},{"cell_type":"markdown","source":["Another way to prepare your sequences for prepadding is to set the TextVectorization to output a ragged tensor. This means the output will not be automatically post-padded. See the output sequences here."],"metadata":{"id":"0xkPlh_qEWl7"}},{"cell_type":"code","source":["# Set the layer to output a ragged tensor\n","vectorize_layer = tf.keras.layers.TextVectorization(ragged=True)\n","\n","# Compute the vocabulary\n","vectorize_layer.adapt(sentences)\n","\n","# Apply the layer to the sentences\n","ragged_sequences = vectorize_layer(sentences)\n","\n","# Print the results\n","print(ragged_sequences)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b5PcG_twEedq","executionInfo":{"status":"ok","timestamp":1739966249227,"user_tz":240,"elapsed":412,"user":{"displayName":"Luis Alfredo Hung Araque","userId":"00964424177241549147"}},"outputId":"ba6abfe6-61e7-4ab8-e93b-e09c82499222"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<tf.RaggedTensor [[6, 3, 2, 4], [6, 3, 2, 10], [5, 3, 2, 4], [9, 5, 7, 2, 4, 8, 11]]>\n"]}]},{"cell_type":"markdown","source":["The main difference between declaring a TextVectorization layer without specifying the ragged argument and declaring it with ragged=True lies in the type of output the layer produces and how input data is expected to be handled.\n","\n","1. Without specifying ragged: When the ragged argument is not specified, the TextVectorization layer will produce an output of type tf.Tensor. This means the output will be a regular tensor with a fixed shape and no support for variable sequence lengths.\n","2. Specifying ragged=True: By declaring the layer with ragged=True, the TextVectorization layer will produce an output of type tf.RaggedTensor. A tf.RaggedTensor is a\n","\n","TensorFlow data structure designed to efficiently handle sequences with variable lengths.\n","Therefore, the main difference lies in how input and output sequences are handled:\n","\n","1. Without ragged, the layer expects fixed-length sequences and produces regular tensors as output.\n","2. With ragged=True, the layer can handle variable-length sequences and produces tf.RaggedTensor as output, which is useful when working with textual data where sequence lengths can vary."],"metadata":{"id":"nnVPzMsEFK47"}},{"cell_type":"markdown","source":["With that, you can now pass it directly to the `pad_sequences()` utility."],"metadata":{"id":"Nobx6xr1FKih"}},{"cell_type":"code","source":["# Pre-pad the sequences in the ragged tensor\n","sequences_pre = tf.keras.utils.pad_sequences(ragged_sequences.numpy())\n","\n","# Print the results\n","print(sequences_pre)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3PNC3OawFvnS","executionInfo":{"status":"ok","timestamp":1739966655285,"user_tz":240,"elapsed":1103,"user":{"displayName":"Luis Alfredo Hung Araque","userId":"00964424177241549147"}},"outputId":"41df3ac4-dfe3-454a-f5d1-9dc7f80a1355"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[ 0  0  0  6  3  2  4]\n"," [ 0  0  0  6  3  2 10]\n"," [ 0  0  0  5  3  2  4]\n"," [ 9  5  7  2  4  8 11]]\n"]}]},{"cell_type":"markdown","source":["## Out-of-vocabulary tokens\n","\n","Lastly, you'll see what the other special token is for. The layer will use the token index `1` when you have input words that are not found in the vocabulary list. For example, you may decide to collect more text after your initial training and decide to not recompute the vocabulary. You will see this in action in the cell below. Notice that the token `1` is inserted for words that are not found in the list."],"metadata":{"id":"mjxQF4ADFvX6"}},{"cell_type":"code","source":["# Try with words that are not in the vocabulary\n","sentences_with_oov = [\n","    'i really love my dog',\n","    'my dog loves my manatee'\n","]\n","\n","# Generate the sequences\n","sequences_with_oov = vectorize_layer(sentences_with_oov)\n","\n","# Print the integer sequences\n","for sentence, sequence in zip(sentences_with_oov, sequences_with_oov):\n","  print(f'{sentence} ---> {sequence}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UIcvCQ09F1Rp","executionInfo":{"status":"ok","timestamp":1739966660377,"user_tz":240,"elapsed":433,"user":{"displayName":"Luis Alfredo Hung Araque","userId":"00964424177241549147"}},"outputId":"2e1564a2-1eae-453e-e3ec-eefa9d247f0a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["i really love my dog ---> [6 1 3 2 4]\n","my dog loves my manatee ---> [2 4 1 2 1]\n"]}]},{"cell_type":"markdown","source":["This concludes another introduction to text data preprocessing. So far, you've just been using dummy data. In the next exercise, you will be applying the same concepts to a real-world and much larger dataset."],"metadata":{"id":"22-IWcRiF7z6"}}]}